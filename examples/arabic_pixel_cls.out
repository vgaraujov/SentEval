SLURM_JOB_ID: 61093540
SLURM_JOB_USER: vsc35188
SLURM_JOB_ACCOUNT: lp_lagom
SLURM_JOB_NAME: pixel_probe
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu
SLURM_NNODES: 1
SLURM_NODELIST: k28g32
SLURM_JOB_CPUS_PER_NODE: 18
SLURM_JOB_GPUS: 3
Date: Wed Jan 17 19:47:22 CET 2024
Walltime: 00-03:00:00
========================================================================
2024-01-17 19:47:38,203 : Starting new HTTPS connection (1): huggingface.co:443
2024-01-17 19:47:38,358 : https://huggingface.co:443 "HEAD /Team-PIXEL/pixel-base/resolve/main/text_renderer_config.json HTTP/1.1" 200 0
2024-01-17 19:47:38,361 : loading text renderer configuration file https://huggingface.co/Team-PIXEL/pixel-base/resolve/main/text_renderer_config.json from cache at /data/leuven/351/vsc35188/.cache/huggingface/transformers/892d6a02d7c441000de399de59ed70d943a81f7b0f536523b4af1111677a8508.e332b34c9c05756dd4aa51d8fa33461dbd79604752296d185f03f8004db30700
2024-01-17 19:47:38,363 : Starting new HTTPS connection (1): huggingface.co:443
2024-01-17 19:47:38,513 : https://huggingface.co:443 "HEAD /Team-PIXEL/pixel-base/resolve/main/GoNotoCurrent.ttf HTTP/1.1" 302 0
2024-01-17 19:47:38,515 : loading font file https://huggingface.co/Team-PIXEL/pixel-base/resolve/main/GoNotoCurrent.ttf from cache at /data/leuven/351/vsc35188/.cache/huggingface/transformers/49e6dc219d1a1a1c9236acaf05a48b542002016a6dc877ee72baab085a84257b.3f28e7f4b38e1efe1b6da4a3732404c19d4c6a614ff32dce90a251e293d4ce58
2024-01-17 19:47:38,516 : Loading font from /data/leuven/351/vsc35188/.cache/huggingface/transformers/49e6dc219d1a1a1c9236acaf05a48b542002016a6dc877ee72baab085a84257b.3f28e7f4b38e1efe1b6da4a3732404c19d4c6a614ff32dce90a251e293d4ce58
2024-01-17 19:47:38,517 : Text renderer PyGameTextRenderer {
  "background_color": "white",
  "dpi": 120,
  "font_color": "black",
  "font_file": "49e6dc219d1a1a1c9236acaf05a48b542002016a6dc877ee72baab085a84257b.3f28e7f4b38e1efe1b6da4a3732404c19d4c6a614ff32dce90a251e293d4ce58",
  "font_size": 8,
  "max_seq_length": 529,
  "pad_size": 3,
  "pixels_per_patch": 16,
  "text_renderer_type": "PyGameTextRenderer"
}

2024-01-17 19:47:38,518 : Starting new HTTPS connection (1): huggingface.co:443
2024-01-17 19:47:38,653 : https://huggingface.co:443 "HEAD /Team-PIXEL/pixel-base/resolve/main/config.json HTTP/1.1" 200 0
2024-01-17 19:47:38,658 : Starting new HTTPS connection (1): huggingface.co:443
2024-01-17 19:47:38,790 : https://huggingface.co:443 "HEAD /Team-PIXEL/pixel-base/resolve/main/pytorch_model.bin HTTP/1.1" 302 0
Some weights of the model checkpoint at Team-PIXEL/pixel-base were not used when initializing ViTModel: ['decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.mask_token', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.7.layernorm_before.weight']
- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ViTModel were not initialized from the model checkpoint at Team-PIXEL/pixel-base and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-01-17 19:47:40,680 : ***** (Probing) Transfer task : ASPECT classification *****
2024-01-17 19:47:40,777 : Loaded 5246 train - 832 dev - 589 test for Aspect
2024-01-17 19:47:40,777 : Computing embeddings for train/dev/test
2024-01-17 20:19:04,774 : Computed embeddings
2024-01-17 20:19:04,775 : Training pytorch-MLP-nhid0-adam-bs64 with standard validation..
2024-01-17 20:19:11,277 : [('reg:1e-05', 74.04), ('reg:0.0001', 74.04), ('reg:0.001', 73.92), ('reg:0.01', 73.56)]
2024-01-17 20:19:11,277 : Validation : best param found is reg = 1e-05 with score             74.04
2024-01-17 20:19:11,277 : Evaluating...
2024-01-17 20:19:12,944 : 
Dev acc : 74.0 Test acc : 71.0 for ASPECT classification

2024-01-17 20:19:12,946 : ***** (Probing) Transfer task : CASE classification *****
2024-01-17 20:19:13,093 : Loaded 5992 train - 903 dev - 669 test for Case
2024-01-17 20:19:13,102 : Computing embeddings for train/dev/test
slurmstepd: error: *** JOB 61093540 ON k28g32 CANCELLED AT 2024-01-17T21:16:49 ***
